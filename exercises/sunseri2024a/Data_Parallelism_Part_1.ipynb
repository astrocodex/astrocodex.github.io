{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85be34e4-1ea6-412d-af7b-bc61aea9a804",
   "metadata": {},
   "source": [
    "# Introduction to Data Parallelism\n",
    ":::{admonition} [James Sunseri](https://orcid.org/0000-0003-4274-2662), [Josh Borrow](https://orcid.org/0000-0002-1327-1921), [Kai-Feng Chen](https://orcid.org/0000-0002-3839-0230)\n",
    ":class: author\n",
    "\n",
    "\n",
    "*Description*: A short walkthrough on using parallelization in python \n",
    "\n",
    "*Intended Audience*: Intermediate Undergraduate\n",
    "\n",
    "*tags*: `libraries:numpy`, `concurrent.futures`, `parallelization`,`parallel-programming`\n",
    "\n",
    "\n",
    "*Last Updated: July 23, 2024*\n",
    ":::\n",
    "\n",
    "\n",
    ":::{admonition} Learning Objectives\n",
    ":class: learningobjective\n",
    "1. Understand how parallelization can improve code runtime \n",
    "2. Implement basic parallization in Python\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541aa6b2-cf08-4e78-b0b9-ff00fc466af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091e262-a721-4be7-b281-5ea92025bcfe",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "This notebook is intended for students who are familiar with the basic principles of scientific programming and are looking to branch into intermediate topics that could be useful for research applications.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fba959-9249-45c6-8005-5db6e91d106f",
   "metadata": {},
   "source": [
    "## Motivation & Goals\n",
    "\n",
    "Often in astronomy, we are working with many pieces of data that are similar in format (i.e. photometric images, spectra, simulation snapshots, etc...) and we want to perform operations across all of these data. Those operations could be simply loading in the data into memory from storage, or they could be as complicated as running a full scale photometric reduction pipeline. In this notebook we start addressing various techniques to speed up working with large amounts of data through parallelism schemes. \n",
    "\n",
    "The goal of this resource is to show someone how to simply parallelize a `for` loop in the context of working with data. This resource uses `concurrent.futures`. \n",
    "\n",
    "We will be using images of galaxies from [Galaxy Zoo](https://zenodo.org/records/3565489#.Y3vFKS-l0eY) for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c85fb-c5bd-4ee0-bb04-9cd4b8ca0452",
   "metadata": {},
   "source": [
    "<p align=\"center\"> \n",
    "    <img src=\"https://thumbnails.zooniverse.org/600x/panoptes-uploads.zooniverse.org/subject_location/520aa31c-6dbe-48de-a281-47ccfbc8f944.jpeg\" width=\"60%\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5333b6-e193-47a7-9e5b-d0247fe72100",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1501b88-286a-4d35-a197-753dfe04d602",
   "metadata": {},
   "source": [
    "We need to load in 1000 2D photometric images from storage into memory using a simple for loop. We can imagine that the time to load in each image takes ~0.01 seconds. The images are stored in the `./images/` directory and have a `.npy` file format. Each image has shape `(424,424)`. We time how long this takes for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e03b0e-56a1-486b-a80c-ccf253eacc39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in the data took ~20.38 seconds\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    image_i = plt.imread(f'./images/{i}.jpg')\n",
    "    time.sleep(0.01)\n",
    "    images.append(image_i)\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Loading in the data took ~{stop - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ecba2-0801-4c3e-ad5b-908d0a948de0",
   "metadata": {},
   "source": [
    "This is very slow because it is being done sequentially with only a single process. Since the 2010's, it has become common place that laptops used for school, research, and everyday use have several cores (4+) at their disposal. A simple use case of parallelism would be to distribute the above `for` loop task across multiple cores to speed up the total time. In the case of ~10 seconds total this is not super useful, but for exceedingly large files or for systems with slow File I/O speeds, this can save minutes to hours of time. \n",
    "\n",
    "For this resource, we will show one of the simplest methods for parallelization of a `for` loop: `concurrent.futures`\n",
    "\n",
    "To use simple parallelization with `concurrent.futures`, it is important to first understand the concept of the `map()` function. The `map()` function returns a map object of the results after applying the given function to each item of a given iterable (i.e. list, tuple, array, etc...). To use the `map()` function we must first rewrite the operations done in the above `for` loop as a mappable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e652113-7a4c-4d90-baef-5ae99bf68c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(i):\n",
    "    \"\"\"\n",
    "    a mappable function to load in the ith image.\n",
    "    \n",
    "    Args:\n",
    "    i - int - index\n",
    "    \n",
    "    Returns:\n",
    "    image_i - np.ndarray - a (424,424) image array corresponding to index i.\n",
    "    \"\"\"\n",
    "    \n",
    "    time.sleep(0.01) #artificial sleep to mimick the scenario\n",
    "    image_i = plt.imread(f'./images/{i}.jpg')\n",
    "    \n",
    "    return image_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01379913-aaa7-42b6-a45c-2964307c90e4",
   "metadata": {},
   "source": [
    "From here we can perform the same task as the initial `for` loop with our mappable function by calling the `map()` function, giving the mappable function and the iteratable we wish to map the function onto. In this case our mappable function is `load_data()` and our iteratable is the same iterable we were going to use in our `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a3f6ff-e6b9-466b-a12b-739fb36285eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in the data took ~20.77 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "images = list(map(load_data, range(1000)))\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Loading in the data took ~{stop - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809bec8-8a5c-4878-bc3d-ef93d85afc7a",
   "metadata": {},
   "source": [
    "::::{admonition} Question\n",
    ":class: attention\n",
    "We note that if the outer most function call `list()` is not present the code runs nearly instantly, why would it not take ~10 seconds like before? \n",
    "\n",
    ":::{admonition}\n",
    ":class:dropdown tip \n",
    "This is because the `map()` function returns an iterator, not the performed iterations stored in a list. In order to retrieve the results of the mapping, we have to force the iterable to iterate. \n",
    "\n",
    "```python\n",
    "result = list(map(func, iter))\n",
    "```\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40522f-c693-4400-994b-229745ebe98a",
   "metadata": {},
   "source": [
    "We don't yet see any speed up, but this is just because we haven't started using multiple cores yet. This is all done on a single core. Any `for` loop can be recast as a mapping. In some cases a mapping can actually be a little faster than a `for` loop. We will now dive into using a pool of processors to perform the mapping in parallel.\n",
    "\n",
    "Using `concurrent.futures` is quite simple and only requires a few lines of new code. Often times `jupyter` environments can clash with the mechanics going on under the hood, to avoid dealing with these issues we simply use a `jupyter` magic command to write the contents of the cell below to a python script file which we will run in the cell below using another magic command (this is equivalent to just running a `.py` file instead of using your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3bbf88-6888-4d1f-b9be-b0cbe276d571",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map_parallel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile map_parallel.py\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "\n",
    "def load_data(i):\n",
    "    \"\"\"\n",
    "    a mappable function to load in the ith image.\n",
    "    \n",
    "    Args:\n",
    "    i - int - index\n",
    "    \n",
    "    Returns:\n",
    "    image_i - np.ndarray - a (256,256) image array corresponding to index i.\n",
    "    \"\"\"\n",
    "    \n",
    "    time.sleep(0.01) #artificial sleep to mimick the scenario\n",
    "    image_i = plt.imread(f'./images/{i}.jpg')\n",
    "    \n",
    "    return image_i\n",
    "\n",
    "def main():\n",
    "\n",
    "    max_workers = 4 #this should be however many cores you are able to use\n",
    "\n",
    "    start = time.time()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        images = list(executor.map(load_data, range(1000)))\n",
    "        executor.shutdown(wait=True)\n",
    "    stop = time.time()  \n",
    "    print(f\"Loading in the data took ~{stop - start:0.2f} seconds\")\n",
    "        \n",
    "    return images\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    images = main()\n",
    "    \"\"\" \n",
    "    from here the list of images can be used as one \n",
    "    pleases. It can be fed into other functions for \n",
    "    analysis for example. For this example, we\n",
    "    will just save them all into large numpy array \n",
    "    and save that array to local storage.\n",
    "    \"\"\"\n",
    "    images = np.array(images)\n",
    "    np.save('images.npy', images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1410efa3-5028-49f5-9249-87895e897094",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in the data took ~5.03 seconds\n"
     ]
    }
   ],
   "source": [
    "!python map_parallel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1822434e-b6aa-42d3-9dd3-1432cf461dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 424, 424, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we load the images back in, in practice one would perform the analysis\n",
    "in the python script and store the results of the analysis in storage.\n",
    "\"\"\"\n",
    "\n",
    "images = np.load('images.npy')\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e5e2e-1b9f-400a-8231-7245d3a6e545",
   "metadata": {},
   "source": [
    "We can see that when using 4 cores the time taken to load in the data is ~3-4x faster! The reason it is not exactly 4x faster is because one of the processors used in the pool is used to manage the process pool. One can use as many workers in the process pool as they have processors available in their hardware. \n",
    "\n",
    "The `ProcessPoolExecutor` is an abstract object in the `concurrent.futures` module that uses a pool of processes to execute function calls asynchronously. `ProcessPoolExecutor` uses the `multiprocessing` module, which allows it to side-step the Global Interpreter Lock that makes parallelization in python frustratingly overcomplicated but also means that only picklable objects can be executed and returned. It is also possible to use `concurrent.futures` to do multithreading related tasks with a `ThreadPoolExecutor`, for a review on the difference between these two topics we refer to this article on [multiprocessing vs. multithreading](https://www.geeksforgeeks.org/difference-between-multithreading-vs-multiprocessing-in-python/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8116a-14cf-4848-a894-c0e1c22911d3",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f97a7-c3e1-47d9-9841-9eae68666887",
   "metadata": {},
   "source": [
    ":::{exercise}\n",
    ":class: dropdown \n",
    "\n",
    "Write your own python script which uses `concurrent.futures` to run a mock \"analysis\" pipeline on all 1000 images using the `pipeline()` function written below.\n",
    "    \n",
    "```python\n",
    "def pipeline(image):\n",
    "    \"\"\"\n",
    "    a mock pipeline analysis function which we wish \n",
    "    to apply to each image in our stack.\n",
    "    \"\"\"\n",
    "    nx, ny = image.size\n",
    "    \n",
    "    weighted_sum = 0\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            weighted_sum += np.sin(image[i, j])\n",
    "    \n",
    "    return weighted_sum\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63888c4-0f1d-4ecf-90dc-f6332e1629bd",
   "metadata": {},
   "source": [
    ":::{exercise}\n",
    ":class: dropdown \n",
    " \n",
    "Use your own `concurrent.futures` analysis pipeline framework code from the first exercise to make a new analysis pipeline where you compute the median brightness in each color filter. Make a histogram of the median brightness for each filter (R,G,B) across all 1000 images. What do you see in the distributions?\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
